import openai
import toml
import streamlit as st
import embeddings.query as qa_chain
import os
from utils.folder_updater import folder_updater

st.set_page_config(page_title='ChatGPT Assistant', layout='wide', page_icon='ğŸ‹')

#################################################################
##### Loading config
#################################################################
os.environ["http_proxy"]="http://127.0.0.1:7890"
os.environ["https_proxy"]="http://127.0.0.1:7890"

#################################################################
##### Generate response function
#################################################################
def generate_response(question, persist_directory):
	chain = qa_chain.get_chain(persist_directory)
	response = qa_chain.query(chain, question)
	return response

#################################################################
##### Building sidebar
#################################################################
with st.sidebar:
    st.markdown("# Control Panel ğŸ“Œ")
    context_level = st.slider('Context Level ğŸ‘‡', 1, 10, 4, 1)
    temperature = st.slider('Temperature ğŸ‘‡', 0.0, 2.0, 1.0, 0.5)
    top_p = st.slider('Top P ğŸ‘‡', 0.1, 1.0, 1.0, 0.1)
    presence_penalty = st.slider('Presence Penalty ğŸ‘‡', -2.0, 2.0, 0.0, 0.1)
    frequency_penalty = st.slider('Frequence Penalty ğŸ‘‡', -2.0, 2.0, 0.0, 0.1)
	#https://platform.openai.com/docs/api-reference/completions/create
    
    uploaded_files = st.file_uploader("Choose txt files", accept_multiple_files=True)
    for uploaded_file in uploaded_files:
        bytes_data = uploaded_file.read()
        st.write("filename:", uploaded_file.name)
        st.write(bytes_data)

#################################################################
##### Chatbox
#################################################################	
if 'past' not in st.session_state:
	st.session_state['past'] = []

if "generated" not in st.session_state:
    st.session_state["generated"] = []
	
message_log = [{"role": "user", "content": "hi"}]
	
st.header("Welcome to Jeru's CHATBOT ğŸ‹")

fu = folder_updater()
keys_list = fu.get_key_list()


st.session_state.options = st.multiselect(
    'è¯·é€‰æ‹©ä½ è¦å¯¹è¯çš„æ•°æ®é›†:(æœªæ¥å¯å¤šé€‰ï¼Œæš‚æ—¶è¯·å•é€‰)',
    keys_list,

    )
options = st.session_state.options

if (len(options)>0):    
    print('dic is: ', str(fu.get_dict()))
    folder = fu.query_uuid(options[0])
    print('folder is: ', folder)
    persist_directory = f"db/{folder}"

prompt = st.text_input("è¾“å…¥é—®é¢˜åå›è½¦", placeholder="Enter your message here...")


if prompt:
	with st.spinner("Generating response..."):
		message_log.append({"role": "user", "content": prompt})
		#output = generate_response(message_log)
		print("persist_directory: ", persist_directory)
		output = generate_response(prompt, persist_directory)
		message_log.append({"role": "assistant", "content": output})
    	#store the output
		st.session_state['past'].append(prompt)
		st.session_state['generated'].append(output)

if st.session_state['generated']:
	for i in range(len(st.session_state['generated'])-1, -1, -1):
		st.markdown(f'''<div style='background:white;color:black;padding:10px'><b>**AI:**</b> {st.session_state["generated"][i]}</div>''',unsafe_allow_html=True)
		st.markdown(f'''<div style='background:#ddd;color:black;padding:10px'><b>**You:**</b> {st.session_state["past"][i]}</div>''',unsafe_allow_html=True)